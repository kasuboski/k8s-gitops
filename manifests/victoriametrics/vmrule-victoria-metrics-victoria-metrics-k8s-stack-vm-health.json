{
  "apiVersion": "operator.victoriametrics.com/v1beta1",
  "kind": "VMRule",
  "metadata": {
    "labels": {
      "app": "victoria-metrics-k8s-stack",
      "app.kubernetes.io/instance": "victoria-metrics",
      "app.kubernetes.io/managed-by": "Helm",
      "app.kubernetes.io/name": "victoria-metrics-k8s-stack",
      "app.kubernetes.io/version": "v0.28.1",
      "helm.sh/chart": "victoria-metrics-k8s-stack-0.65.1",
      "k8s.joshcorp.co/app": "victoriametrics"
    },
    "name": "victoria-metrics-victoria-metrics-k8s-stack-vm-health",
    "namespace": "victoria-metrics"
  },
  "spec": {
    "groups": [
      {
        "name": "vm-health",
        "params": {},
        "rules": [
          {
            "alert": "TooManyRestarts",
            "annotations": {
              "description": "Job {{ $labels.job }} (instance {{ $labels.instance }}) has restarted more than twice in the last 15 minutes. It might be crashlooping.\n",
              "summary": "{{ $labels.job }} too many restarts (instance {{ $labels.instance }})"
            },
            "expr": "changes(process_start_time_seconds{job=~\".*(victoriametrics|vmselect|vminsert|vmstorage|vmagent|vmalert|vmsingle|vmalertmanager|vmauth).*\"}[15m]) \u003e 2",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "ServiceDown",
            "annotations": {
              "description": "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 2 minutes.",
              "summary": "Service {{ $labels.job }} is down on {{ $labels.instance }}"
            },
            "expr": "up{job=~\".*(victoriametrics|vmselect|vminsert|vmstorage|vmagent|vmalert|vmsingle|vmalertmanager|vmauth).*\"} == 0",
            "for": "2m",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "ProcessNearFDLimits",
            "annotations": {
              "description": "Exhausting OS file descriptors limit can cause severe degradation of the process.\nConsider to increase the limit as fast as possible.\n",
              "summary": "Number of free file descriptors is less than 100 for \"{{ $labels.job }}\"(\"{{ $labels.instance }}\") for the last 5m"
            },
            "expr": "(process_max_fds - process_open_fds) \u003c 100",
            "for": "5m",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "TooHighMemoryUsage",
            "annotations": {
              "description": "Too high memory usage may result into multiple issues such as OOMs or degraded performance.\nConsider to either increase available memory or decrease the load on the process.\n",
              "summary": "It is more than 80% of memory used by \"{{ $labels.job }}\"(\"{{ $labels.instance }}\")"
            },
            "expr": "(min_over_time(process_resident_memory_anon_bytes[10m]) / vm_available_memory_bytes) \u003e 0.8",
            "for": "5m",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "TooHighCPUUsage",
            "annotations": {
              "description": "Too high CPU usage may be a sign of insufficient resources and make process unstable. Consider to either increase available CPU resources or decrease the load on the process.\n",
              "summary": "More than 90% of CPU is used by \"{{ $labels.job }}\"(\"{{ $labels.instance }}\") during the last 5m"
            },
            "expr": "(rate(process_cpu_seconds_total[5m]) / process_cpu_cores_available) \u003e 0.9",
            "for": "5m",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "TooHighGoroutineSchedulingLatency",
            "annotations": {
              "description": "Go runtime is unable to schedule goroutines execution in acceptable time. This is usually a sign of insufficient CPU resources or CPU throttling. Verify that service has enough CPU resources. Otherwise, the service could work unreliably with delays in processing.\n",
              "summary": "\"{{ $labels.job }}\"(\"{{ $labels.instance }}\") has insufficient CPU resources for \u003e15m"
            },
            "expr": "histogram_quantile(0.99, sum(rate(go_sched_latencies_seconds_bucket{job=~\".*(victoriametrics|vmselect|vminsert|vmstorage|vmagent|vmalert|vmsingle|vmalertmanager|vmauth).*\"}[5m])) by(le,job,instance,cluster)) \u003e 0.1",
            "for": "15m",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "TooManyLogs",
            "annotations": {
              "description": "Logging rate for job \\\"{{ $labels.job }}\\\" ({{ $labels.instance }}) is {{ $value }} for last 15m. Worth to check logs for specific error messages.\n",
              "summary": "Too many logs printed for job \"{{ $labels.job }}\" ({{ $labels.instance }})"
            },
            "expr": "sum(increase(vm_log_messages_total{level=\"error\"}[5m])) without(app_version,location) \u003e 0",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "TooManyTSIDMisses",
            "annotations": {
              "description": "Unexpected TSID misses for \\\"{{ $labels.job }}\\\" ({{ $labels.instance }}) for the last 15 minutes.\nIf this happens after unclean shutdown of VictoriaMetrics process (via \\\"kill -9\\\", OOM or power off),\nthen this is OK - the alert must go away in a few minutes after the restart.\nOtherwise this may point to the corruption of index data.\n",
              "summary": "Unexpected TSID misses for job \"{{ $labels.job }}\" ({{ $labels.instance }}) for the last 15 minutes"
            },
            "expr": "increase(vm_missing_tsids_for_metric_id_total[5m]) \u003e 0",
            "for": "15m",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "ConcurrentInsertsHitTheLimit",
            "annotations": {
              "description": "The limit of concurrent inserts on instance {{ $labels.instance }} depends on the number of CPUs.\nUsually, when component constantly hits the limit it is likely the component is overloaded and requires more CPU.\nIn some cases for components like vmagent or vminsert the alert might trigger if there are too many clients\nmaking write attempts. If vmagent's or vminsert's CPU usage and network saturation are at normal level, then \nit might be worth adjusting `-maxConcurrentInserts` cmd-line flag.\n",
              "summary": "{{ $labels.job }} on instance {{ $labels.instance }} is constantly hitting concurrent inserts limit"
            },
            "expr": "avg_over_time(vm_concurrent_insert_current[1m]) \u003e= vm_concurrent_insert_capacity",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "IndexDBRecordsDrop",
            "annotations": {
              "description": "VictoriaMetrics could skip registering new timeseries during ingestion if they fail the validation process. \nFor example, `reason=too_long_item` means that time series cannot exceed 64KB. Please, reduce the number \nof labels or label values for such series. Or enforce these limits via `-maxLabelsPerTimeseries` and \n`-maxLabelValueLen` command-line flags.\n",
              "summary": "IndexDB skipped registering items during data ingestion with reason={{ $labels.reason }}."
            },
            "expr": "increase(vm_indexdb_items_dropped_total[5m]) \u003e 0",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "RowsRejectedOnIngestion",
            "annotations": {
              "description": "Ingested rows on instance \"{{ $labels.instance }}\" are rejected due to the following reason: \"{{ $labels.reason }}\"",
              "summary": "Some rows are rejected on \"{{ $labels.instance }}\" on ingestion attempt"
            },
            "expr": "rate(vm_rows_ignored_total[5m]) \u003e 0",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "TooHighQueryLoad",
            "annotations": {
              "description": "Instance {{ $labels.instance }} ({{ $labels.job }}) is failing to serve read queries during last 15m.\nConcurrency limit `-search.maxConcurrentRequests` was reached on this instance and extra queries were\nput into the queue for `-search.maxQueueDuration` interval. But even after waiting in the queue these queries weren't served.\nThis happens if instance is overloaded with the current workload, or datasource is too slow to respond.\nPossible solutions are the following:\n* reduce the query load;\n* increase compute resources or number of replicas;\n* adjust limits `-search.maxConcurrentRequests` and `-search.maxQueueDuration`.\nSee more at https://docs.victoriametrics.com/victoriametrics/troubleshooting/#slow-queries.\n",
              "summary": "Read queries fail with timeout for {{ $labels.job }} on instance {{ $labels.instance }}"
            },
            "expr": "increase(vm_concurrent_select_limit_timeout_total[5m]) \u003e 0",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          }
        ]
      }
    ]
  }
}
