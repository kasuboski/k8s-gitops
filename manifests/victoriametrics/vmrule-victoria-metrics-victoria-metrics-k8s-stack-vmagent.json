{
  "apiVersion": "operator.victoriametrics.com/v1beta1",
  "kind": "VMRule",
  "metadata": {
    "labels": {
      "app": "victoria-metrics-k8s-stack",
      "app.kubernetes.io/instance": "victoria-metrics",
      "app.kubernetes.io/managed-by": "Helm",
      "app.kubernetes.io/name": "victoria-metrics-k8s-stack",
      "app.kubernetes.io/version": "v0.28.1",
      "helm.sh/chart": "victoria-metrics-k8s-stack-0.65.1",
      "k8s.joshcorp.co/app": "victoriametrics"
    },
    "name": "victoria-metrics-victoria-metrics-k8s-stack-vmagent",
    "namespace": "victoria-metrics"
  },
  "spec": {
    "groups": [
      {
        "concurrency": 2,
        "interval": "30s",
        "name": "vmagent",
        "params": {},
        "rules": [
          {
            "alert": "PersistentQueueIsDroppingData",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=49\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Vmagent dropped {{ $value | humanize1024 }} from persistent queue on instance {{ $labels.instance }} for the last 10m.",
              "summary": "Instance {{ $labels.instance }} is dropping data from persistent queue"
            },
            "expr": "sum(increase(vm_persistentqueue_bytes_dropped_total[5m])) without(path) \u003e 0",
            "for": "10m",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "RejectedRemoteWriteDataBlocksAreDropped",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=79\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Job \"{{ $labels.job }}\" on instance {{ $labels.instance }} drops the rejected by remote-write server data blocks. Check the logs to find the reason for rejects.",
              "summary": "Vmagent is dropping data blocks that are rejected by remote storage"
            },
            "expr": "sum(increase(vmagent_remotewrite_packets_dropped_total[5m])) without(url) \u003e 0",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "TooManyScrapeErrors",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=31\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Job \"{{ $labels.job }}\" on instance {{ $labels.instance }} fails to scrape targets for last 15m",
              "summary": "Vmagent fails to scrape one or more targets"
            },
            "expr": "increase(vm_promscrape_scrapes_failed_total[5m]) \u003e 0",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "ScrapePoolHasNoTargets",
            "annotations": {
              "description": "Vmagent \"{{ $labels.job }}\" has scrape_pool \"{{ $labels.scrape_job }}\" with 0 discovered targets. It is likely a misconfiguration. Please follow https://docs.victoriametrics.com/victoriametrics/vmagent/#debugging-scrape-targets to troubleshoot the scraping config.",
              "summary": "Vmagent has scrape_pool with 0 configured/discovered targets"
            },
            "expr": "sum(vm_promscrape_scrape_pool_targets) without(status,instance,pod) == 0",
            "for": "30m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "TooManyWriteErrors",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=77\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Job \"{{ $labels.job }}\" on instance {{ $labels.instance }} responds with errors to write requests for last 15m.",
              "summary": "Vmagent responds with too many errors on data ingestion protocols"
            },
            "expr": "(sum(increase(vm_ingestserver_request_errors_total[5m])) without(name,net,type) + sum(increase(vmagent_http_request_errors_total[5m])) without(path,protocol)) \u003e 0",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "TooManyRemoteWriteErrors",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=61\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Vmagent fails to push data via remote write protocol to destination \"{{ $labels.url }}\"\n Ensure that destination is up and reachable.",
              "summary": "Job \"{{ $labels.job }}\" on instance {{ $labels.instance }} fails to push to remote storage"
            },
            "expr": "rate(vmagent_remotewrite_retries_count_total[5m]) \u003e 0",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "RemoteWriteConnectionIsSaturated",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=84\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "The remote write connection between vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }}) and destination \"{{ $labels.url }}\" is saturated by more than 90% and vmagent won't be able to keep up.\n There could be the following reasons for this:\n * vmagent can't send data fast enough through the existing network connections. Increase `-remoteWrite.queues` cmd-line flag value to establish more connections per destination.\n * remote destination can't accept data fast enough. Check if remote destination has enough resources for processing.",
              "summary": "Remote write connection from \"{{ $labels.job }}\" (instance {{ $labels.instance }}) to {{ $labels.url }} is saturated"
            },
            "expr": "(rate(vmagent_remotewrite_send_duration_seconds_total[5m]) / vmagent_remotewrite_queues) \u003e 0.9",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "PersistentQueueForWritesIsSaturated",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=98\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Persistent queue writes for vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }}) are saturated by more than 90% and vmagent won't be able to keep up with flushing data on disk. In this case, consider to decrease load on the vmagent or improve the disk throughput.",
              "summary": "Persistent queue writes for instance {{ $labels.instance }} are saturated"
            },
            "expr": "rate(vm_persistentqueue_write_duration_seconds_total[5m]) \u003e 0.9",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "PersistentQueueForReadsIsSaturated",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=99\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Persistent queue reads for vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }}) are saturated by more than 90% and vmagent won't be able to keep up with reading data from the disk. In this case, consider to decrease load on the vmagent or improve the disk throughput.",
              "summary": "Persistent queue reads for instance {{ $labels.instance }} are saturated"
            },
            "expr": "rate(vm_persistentqueue_read_duration_seconds_total[5m]) \u003e 0.9",
            "for": "15m",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "SeriesLimitHourReached",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=88\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Max series limit set via -remoteWrite.maxHourlySeries flag is close to reaching the max value. Then samples for new time series will be dropped instead of sending them to remote storage systems.",
              "summary": "Instance {{ $labels.instance }} reached 90% of the limit"
            },
            "expr": "(vmagent_hourly_series_limit_current_series / vmagent_hourly_series_limit_max_series) \u003e 0.9",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "SeriesLimitDayReached",
            "annotations": {
              "dashboard": "http://grafana.domain.com/d/G7Z9GzMGz?viewPanel=90\u0026var-instance={{ $labels.instance }}\u0026var-cluster={{ $labels.cluster }}",
              "description": "Max series limit set via -remoteWrite.maxDailySeries flag is close to reaching the max value. Then samples for new time series will be dropped instead of sending them to remote storage systems.",
              "summary": "Instance {{ $labels.instance }} reached 90% of the limit"
            },
            "expr": "(vmagent_daily_series_limit_current_series / vmagent_daily_series_limit_max_series) \u003e 0.9",
            "labels": {
              "severity": "critical"
            }
          },
          {
            "alert": "ConfigurationReloadFailure",
            "annotations": {
              "description": "Configuration hot-reload failed for vmagent on instance {{ $labels.instance }}. Check vmagent's logs for detailed error message.",
              "summary": "Configuration reload failed for vmagent instance {{ $labels.instance }}"
            },
            "expr": "(vm_promscrape_config_last_reload_successful != 1) or (vmagent_relabel_config_last_reload_successful != 1)",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "StreamAggrFlushTimeout",
            "annotations": {
              "description": "Stream aggregation process can't keep up with the load and might produce incorrect aggregation results. Check logs for more details. Possible solutions: increase aggregation interval; aggregate smaller number of series; reduce samples' ingestion rate to stream aggregation.",
              "summary": "Streaming aggregation at \"{{ $labels.job }}\" (instance {{ $labels.instance }}) can't be finished within the configured aggregation interval."
            },
            "expr": "increase(vm_streamaggr_flush_timeouts_total[5m]) \u003e 0",
            "labels": {
              "severity": "warning"
            }
          },
          {
            "alert": "StreamAggrDedupFlushTimeout",
            "annotations": {
              "description": "Deduplication process can't keep up with the load and might produce incorrect results. Check docs https://docs.victoriametrics.com/victoriametrics/stream-aggregation/#deduplication and logs for more details. Possible solutions: increase deduplication interval; deduplicate smaller number of series; reduce samples' ingestion rate.",
              "summary": "Deduplication \"{{ $labels.job }}\" (instance {{ $labels.instance }}) can't be finished within configured deduplication interval."
            },
            "expr": "increase(vm_streamaggr_dedup_flush_timeouts_total[5m]) \u003e 0",
            "labels": {
              "severity": "warning"
            }
          }
        ]
      }
    ]
  }
}
